{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data:preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJUgpmgf0-O_"
      },
      "source": [
        "**Data Preprocessing** <br/>\n",
        "This Notebook is responsible for saving the vocabulary, train, val and test data for its subsequent use in the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt34oQKwjmer",
        "outputId": "ff47ccd8-a56a-437a-cf21-fafbe133bea1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MCUtprckOUE"
      },
      "source": [
        "def extract_phrases(my_tree, phrase):\n",
        "  \"\"\"\n",
        "  This method is extract the valid phrases according to the grammar from the parse tree of a sentence.\n",
        "  Parameters\n",
        "  ----------\n",
        "  my_tree: This is the parse tree of the sentence\n",
        "  phrases: Non terminals of the grammar rules. This basically defines the structure of phrases we are trying to extract\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  a list of tokens \n",
        "  \"\"\"\n",
        "\n",
        "  my_phrases = []\n",
        "  if my_tree.label() in phrase:\n",
        "\n",
        "    my_phrases.append(my_tree.copy(True))\n",
        "\n",
        "  for child in my_tree:\n",
        "\n",
        "    if type(child) is nltk.Tree:\n",
        "\n",
        "      list_of_phrases = extract_phrases(child, phrase)\n",
        "      if len(list_of_phrases) > 0:\n",
        "        my_phrases.extend(list_of_phrases)\n",
        "  \n",
        "  return my_phrases\n",
        "\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "  \"\"\"\n",
        "  This method tokenizes the input text\n",
        "  Parameters\n",
        "  ----------\n",
        "  text: The sentence to be tokenized\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  a list of tokens \n",
        "  \"\"\"\n",
        "  grammar = \"\"\"NP: {<RB>*<DT>?(<JJ>|<JJS>|<JJR>)*(<NN>|<NNP>|<NNS>)+}\n",
        "               RBJJ:{(<RB>|<RBR>|<RBS>)+(<JJ>|<JJS>|<JJR>)+}\n",
        "               JJ: {<JJ>}\n",
        "               JJS: {<JJS>}\n",
        "               JJR: {<JJR>}\n",
        "               VB: {<VB>}\n",
        "               VBG: {<VBG>}\n",
        "               VBN: {<VBN>}\n",
        "               VBP: {<VBP>}\n",
        "               VBZ: {<VBZ>}\n",
        "               VBD: {<VBD>}\n",
        "               MD: {<MD>}\n",
        "               RB: {<RB>}\n",
        "               RBR: {<RBR>}\n",
        "               RBS: {<RBS>}\n",
        "               PRP: {<PRP>}\n",
        "               IN: {<IN>}\n",
        "               CC: {<CC>}\n",
        "                \"\"\"\n",
        "  cp = nltk.RegexpParser(grammar)\n",
        "  sentence = nltk.pos_tag(nltk.tokenize.word_tokenize(text))\n",
        "  tree = cp.parse(sentence)\n",
        "  list_of_noun_phrases = extract_phrases(tree, ['NP','VBD','IN','VB','VBN','VBP','VBZ','RBR','RB','RBS','PRP','JJ','JJS','JJR','RBJJ','CC'])\n",
        "  tokens=[]\n",
        "  for phrase in list_of_noun_phrases:\n",
        "    tokens.append(\"_\".join([x[0] for x in phrase.leaves()]))\n",
        "  return tokens "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6RZuoPykT-y"
      },
      "source": [
        "#Using the custom tokenizer to tokenize our text\n",
        "TEXT = torchtext.data.Field(tokenize=custom_tokenizer,lower=True)\n",
        "LABEL = torchtext.data.LabelField(dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXq7uB6rkgX0"
      },
      "source": [
        "train_data,val_test_data = torchtext.data.TabularDataset(\n",
        "    path='IMDB Dataset.csv',\n",
        "    format=\"CSV\",\n",
        "    fields=[('review',TEXT),('sentiment',LABEL)],\n",
        "    skip_header = True\n",
        ").split(0.8)\n",
        "\n",
        "val_data,test_data=val_test_data.split(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwCWnHvMqoOp"
      },
      "source": [
        "TEXT.build_vocab(train_data,max_size=200000, vectors = \"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w6DPa3ds48q"
      },
      "source": [
        "weights = TEXT.vocab.vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Psfu-ws7OO"
      },
      "source": [
        "def create_embeddings(word2idx, weights,embeddingbag):\n",
        "  \"\"\"\n",
        "  This method is used to create embedding representations for the phrases.\n",
        "  Parameters\n",
        "  ----------\n",
        "  word2idx: A mapping from words to index\n",
        "  weights: GloVe word vectors for individual words.\n",
        "  embeddingbag: an instance of EmbeddingBag to average the word vectors.\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  word embeddings for all the words including the phrases. \n",
        "  \"\"\"\n",
        "  new_weights=weights.detach().clone()\n",
        "  for word,index in list(word2idx.items()):\n",
        "    if '_' in word:\n",
        "      tokens = word.split('_')\n",
        "      token_id=[]\n",
        "      for token in tokens:\n",
        "        token_id.append(word2idx[token])\n",
        "      inputs = torch.LongTensor([token_id])\n",
        "      new_vec = embeddingbag(inputs)\n",
        "      new_weights[index] = new_vec\n",
        "      token_id=[]\n",
        "  return new_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ows5H8b3s-Ae"
      },
      "source": [
        "embeddingbag = nn.EmbeddingBag.from_pretrained(weights)\n",
        "modified_embeddings = create_embeddings(TEXT.vocab.stoi,weights,embeddingbag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S257gKZdtAXR"
      },
      "source": [
        "TEXT.vocab.set_vectors(TEXT.vocab.stoi,modified_embeddings,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJNAP7txL1AF"
      },
      "source": [
        "**Saving the vocabulary and the data for subsequent use.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv-SpFjPtQZ5"
      },
      "source": [
        "import dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVXO-IoRtOn9"
      },
      "source": [
        "with open(\"/content/vocab\",\"wb\")as f:\n",
        "     dill.dump(TEXT,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAo-vGU1w5a3"
      },
      "source": [
        "with open(\"/content/label\",\"wb\")as f:\n",
        "     dill.dump(LABEL,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vb6CHpYt97w"
      },
      "source": [
        "with open(\"/content/vocab\",\"rb\")as f:\n",
        "     TEXT1=dill.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcQVaHqlwtvf"
      },
      "source": [
        "with open(\"/content/train_data\",\"wb\")as f:\n",
        "     dill.dump(train_data.examples,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTwEov8Qn1aC"
      },
      "source": [
        "with open(\"/content/fields\",\"wb\")as f:\n",
        "     dill.dump(train_data.fields,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1wuAn5QFleR"
      },
      "source": [
        "with open(\"/content/val_data\",\"wb\")as f:\n",
        "     dill.dump(val_data.examples,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohh6Z1x6Fqdb"
      },
      "source": [
        "with open(\"/content/test_data\",\"wb\")as f:\n",
        "     dill.dump(test_data.examples,f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}